

trump_tweets.csv (raw download, 6 years)
trump_clean_tweets.csv (cleaned up),
timestamp (datetime), day (number, 0-2190), week (number, 0-313), tweet (string), polarity (+/-) // DONE

-> generation (SEMIH - PerÅŸembe)
code: parsing operation
generate:

trumps_words.csv // DONE
words (string), frequency (number)

trump_mentions.csv // DONE
words (string), frequency (number)

-> handmade (KAZIM) // DONE
trump_adjectives.csv (keep the old version, pick from trump_words.csv, fill polarity)
words (string), frequency (number), polarity (+/-)

trump_actions.csv (keep the old version, pick from trump_words.csv, fill polarity) // POSTPONED
words (string), frequency (number)

trump_actors_rated.csv (keep the old version, pick from trump_words.csv, fill polarity)words words (string), frequency (number), alliance (enemy/friend) // DONE

-> sentiment analysis (BERAT) // DONE
code: "group tweets by timestamp per day"
input: "trump_clean_tweets.csv"
generate:
trump_positive_tweets_daily.csv (2.190 line)
trump_negative_tweets_daily.csv (2.190 line)
365 * 6 = 2.190 data points (per day)

1. Milestone

-> enemy breadth analysis in time // DONE
code: trump_actors_rated.csv search per word in wordlist
generate: "trump_actors_weekly.csv"
week (0-313), enemy (array ([enemy_name:enemy_freq],[enemy_name:enemy_freq],[enemy_name:enemy_freq]))
2.190 / 7 ~= 313 data points (per week)

trump_actors_weekly will be divided into two, enemies and allies. // TODO

-> most hated actors in time // Removed from the plan
-> most loved actors in time // Removed from the plan
code: parse trump_actors_weekly.csv and calculate cumulative freq
generate: "trump_nemesises_weekly.csv" and "trump_allies_weekly.csv"
week (0-313), enemy1_freq (number), enemy2_freq (number), enemy3_freq (number) 

2. Milestone

-> Gephy graph
code: parse "trump_nemesises_weekly.csv"
generate: nXn symmetric matrix // DONE
import Gephy

25x25 matrix enemy relation with other enemies, cumulative

3. Milestone

Graph (KAZIM)


Mentions Regex
(@[A-Za-z0-9]+)|